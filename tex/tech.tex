%# -*- coding: utf-8-unix -*-
%%==================================================
%% chapter02.tex for SJTU Master Thesis
%% based on CASthesis
%% modified by wei.jianwen@gmail.com
%% Encoding: UTF-8
%%==================================================

\chapter{相关工作与技术}
\label{chap:example}
本文研究的竞争感知的混合线程放置框架主要基于C-MCS（Cohort MCS ）锁实现，而C-MCS锁是适用于NUMA架构的两层的MCS锁。所以本章主要介绍NUMA架构，MCS锁，C-MCS锁、线程调度/放置及与之相关的技术。另外为了在现有使用Pthread mutex的应用中快速高效地测试本文的研究成果，我们使用liTL框架对本文研究的改进后的cstmcs锁进行了封装，所以本章最后会对LiTL做简要介绍。
\begin{figure}[t]
	\centering
	\includegraphics[width=5.0in]{NUMA.png}
	\caption{由四个四核处理器组成的NUMA系统}
	\label{Fig:numa}
\end{figure}
\section{NUMA架构及相关优化技术}

大型现代服务器通常由多个处理器节点组成，每个节点包含一个多核处理器和一块本地内存，其中本地内存上一般会有一个或多个内存控制器（memory controller）来处理来自本地节点或者其他节点上的核的内存访问请求，如图\ref{Fig:numa}所示。所有计算节点之间通过高速通信介质（interconnect）连接成单个缓存一致性系统，虽然物理内存被分割在了多个节点上，但是逻辑上物理地址空间仍然是全局共享的，也就是说所有核能够透明地访问所有节点上地物理内存。计算核心直接通过本地内存控制器来访问本地内存，而访问远程内存时则要通过节点间通信介质和远程内存的内存控制器。访问远程内存通常要比访问本地内存花费更多的时间，所以这些系统都具有非一致性内存访问时间（Non-Uniform Memory access, NUMA）的特征。再考虑到内存和缓存地层级化（memory hierarchy）设计，使得访存的非一致性更加显著。

显然要使应用程序在NUMA架构上获得最佳的性能，在放置应用程序的线程和内存数据时必须考虑系统资源的物理构成和分布，比如为了充分利用访存操作的局部性，将线程及其访问的数据放置在同一个node上来避免远程内存访问的巨大开销。由于NUMA架构的服务器的广泛应用，目前很多操作系统都针对NUMA因素做了通用的优化，比如很多Linux系统都提供了numactl用于查看和优化NUMA系统中的线程放置和内存管理。使用numactl中的numastat用户可以查看各个NUMA节点的内存分配和节点之间的内存访问状况，比如查看每个节点上运行的进程在该节点和其他节点上各申请了多少内存。通过设置numactl的参数用户可以限定应用只能运行在某些特定NUMA节点集合上或者限定该应用的内存只能分配在某些NUMA节点集合上。除此以外，numactl还可以设置更为复杂的内存分配核管理策略，比如是否使用巨页（huge page），内存是否交织（interleave）分布在所有或者某些节点上。

上述操作系统提供的的这些优化工具一方面是非常粗粒度的，另一方面要求用户必须事先了解所运行的程序的特征，而现实的应用特征是复杂多变的而且事先难以预测的，所以仅靠这些通用工具很难最大化应用的性能，进一步的优化必须监测和考虑应用程序本身的特征。Carrefour\cite{dashti2013traffic}在分析了大量应用程序的访存特征后，认为应该根据应用的内存访问特征来使用合适的内存放置和管理策略，如图\ref{Fig:carrefour}所示，Carrefour支持三种内存访问策略：
\begin{itemize}
\item  复制（replication），即将同一个页的拷贝放置在多个NUMA节点上，复制将热点数据的访问压力分摊到了多个内存控制器，同时避免了远程内存访问，但是必须保持多个复制页内容一致，类似于缓存一致性，代价非常高昂，所以一般只对读写比很高的页采用复制策略；
\item  交织（interleaving），即将内存页平均放置在所有节点上，从而平衡各个内存控制器和节点间通信介质的访问压力，操作系统提供的交织是全局性的，而Carrefour中的交织可以只对部分页使用，进行更细力度的内存管理；
\item 协同放置（co-locate），即将共享内存页的线程与其共享的内存页放置在同一个节点上，从而减少远程内存访问和缓存一致性操作。
\end{itemize}

\begin{figure}[t]
	\centering
	\includegraphics[width=4.2in]{Carrefour.pdf}
	\caption{Carrefour中的内存放置策略选择}
	\label{Fig:carrefour}
\end{figure}

\section{MCS锁与C-MCS锁}
\subsection{MCS锁}
MCS锁是一种基于单项链表的高性能、可扩展、公平的自旋锁，由John Mellor-Crummey和Michael Scott在1991年提出， 其名称来源于发明人的名字首字母。

MCS锁中包含一个指向队尾的指针tail，每个竞争者在队列中用一个记录（record）表示，其中每个记录包含一个指向后继节点的指针和一个表示当前是否可以进入关键区域的布尔变量。如图\ref{Fig:MCS}所示，每个线程请求锁时将自己记录中布尔变量设为false，然后使用compare\_and\_swap这个原子操作来完成以下操作将自身的记录加入队列：1）使tail指针指向自己的记录；2）将自己的记录连接在前驱节点（如果有的话）的后面。如果该线程有前驱节点，则它在自身记录的布尔变量上自旋直到其被前驱节点设为true然后进入关键区域，否则该线程将自己的布尔变量设为true直接进入关键区域。放锁时只需要将后继节点的记录中的布尔变量设为true然后断开于后继节点的链接即可，如果没有后继节点则将tail设为null。
\begin{figure}[t]
	\centering
	\includegraphics[width=5.6in]{MCS-lock.png}
	\caption{MCS锁拿放锁示例}
	\label{Fig:MCS}
\end{figure}

MCS的FIFO的公平性是通过显式存在的队列来维持的，其正确性由算法本身和compare\_and\_swap这个原子操作共同保证，每个线程只在本地标志变量上自旋并且只有自身和前驱节点对应的线程会访问该变量，这是MCS锁获得高性能和可扩展性的主要原因。

\subsection{C-MCS锁}
C-MCS锁是针对NUMA架构下内存及缓存访问的非一致性而对MCS锁做的适应性改进。它可以看作是一个两层的 MCS锁。如图\ref{Fig:CSTMCS}所示，该示例中展示了一个有9个线程的多线程应用，每个线程都差不多在同一个时刻竞争同一个锁（具体顺序如图（a）所示），其中线程T1到T5在节点0上而另外四个线程在节点1上，图a表示使用MCS锁时按照FIFO的顺序会有6次跨NUMA节点的锁传递(黄色表示跨节点锁传递)，图b表示如果对锁的传递顺序加以合理调度则可以将跨节点的锁传递次数减为1，从而改善MCS锁在NUMA架构下的性能。a,b两图展示了C-MCS锁和MCS锁的本质差别，即C-MCS通过改变MCS锁的锁调度顺序，减少跨节点的锁传递频率，获得更好的性能。
\begin{figure}[t]
	\centering
	\includegraphics[width=5.0in]{CSTMCS.png}
	\caption{C-MCS锁示例}
	\label{Fig:CSTMCS}
\end{figure}
C-MCS的具体实现还是以MCS锁为基础，包含一个全局MCS锁和每个节点上的一个局部MCS锁。全局MCS锁在所有节点之间共享，它的主要作用时将竞争分隔在各个节点内；而每个局部MCS锁只在其所在的节点内的线程间共享。一个线程只有同时拿到全局MCS锁和其所在节点的局部MCS锁才可以进入关键区域。每个要执行关键区域的线程先竞争本地MCS锁，每个节点上第一个拿到本地MCS锁的线程继续竞争全局MCS，其他线程直接从第一个线程继承全局MCS锁。执行完关键区域的线程先释放本地MCS锁，如果锁在本地节点的传递次数超过了预先设定的上限或者本地节点上没有其他请求者的时候才会释放全局锁。所以C-MCS锁的传递顺序可以描述为，锁的持有者将其传给一个本地的最早请求者当且仅当以下两个条件同时满足：
\begin{itemize}
\item  本地节点当前至少有一个请求者；
\item  其所在本地节点的锁传递次数还未超过预先设定的threshold；
\end{itemize}
其中设置threshold是为了防止深层的不公平。

C-MCS锁在决定锁的调度顺序时考虑了等待队列中的线程和当前拿锁线程在NUMA拓扑上的相对位置，从而让减少了锁在NUMA节点之间的迁移频率，提高锁的总吞吐率。相反的，标准的MCS锁不能感知底层的NUMA因素，所以只能按照线程的到达时间即线程请求锁的时间来决定锁的传递顺序，所以在NUMA架构上会有性能损失，C-MCS锁放弃了MCS锁全局FIOF的锁传递顺序，只保证单个节点内的FIFO锁传递顺序，所以其高吞吐率的获取是以牺牲一定的短期公平性为代价的。

\section{线程调度与放置}
\subsection{NUMA架构下Linux线程调度}
作为资源管理的核心部分，操作系统的线程调度器的主要职责是保证准备好的线程被调度到可用的核上去。在Linux系统当前使用的调度算法CFS（Completely Fair Scheduling）\cite{lozi2016linux}是WFQ（Weighted Fair Queueing）调度算法的一种实现。在单CPU系统中，CFS的实现非常简单。为了实现公平调度，CFS定义了一个固定长度的时间间隔，在该间隔内系统中的每个线程至少运行一次，该间隔被按照线程的权重按比例分为若干个大小不一的时间片（time slice），每个线程的权重就是它的优先级。正在运行的线程会不断增加它的已运行时间（vruntime），当它的vruntime超过分配给其的时间片时，如果当前有其他可以运行的线程，那么正在运行的线程就会被抢占；另外当前运行的线程也可能被另一个被唤醒的vruntime更小的线程抢占。具体的实现中，线程被组织为一个用红黑树实现的运行队列（runqueue），如图\ref{Fig:CFS}所示在该运行队列中，每个线程按照其vruntime排序，当CPU需要找一个线程来运行的时候，红黑树中最左边的线程也就是vruntime最小的线程会被选择。

\begin{figure}[t]
	\centering
	\includegraphics[width=5.0in]{CFS.png}
	\caption{CFS示例}
	\label{Fig:CFS}
\end{figure}

在NUMA架构下，由于缓存一致性和同步机制等的巨大代价及内存访问非一致性等原因的存在使得CFS的实现变得非常复杂。为了保持良好的可扩展性，CFS使用了per-core runqueue，即每个核一个运行队列，这样设计大的主要原因是进程切换（context switch）发生在关键路径上而每个核一个运行队列使得进程切换时只需要访问本地运行队列。但是为了使调度算法在使用了per-core runqueue时能够正确高效地运行，必须有额外的机制来保持运行队列的负载均衡。Linux CFS采用的方法是周期性地运行一个负载均衡算法来保持所有队列负载的大致均衡，具体实现中采用了层级化地策略（hierarchy strategy），所有核被逻辑上组织为一个层次结构，该层次结构的最底部是单个核，这些核在下一层及后面的层被如何分组是由它们对物理资源（内存，各级缓存）的共享拓扑决定的。每层的结构被称为一个调度域（scheduling domain），负载均衡算法按照自下而上的顺序在每个调度域内运行。将所有核组织成一个层次结构然后自下而上运行负载均衡算法相比直接在所有核之间通过线程迁移来调节负载的主要优势在于可以减少跨深层NUMA因素的线程迁移比例从而使得负载均衡算法的开销尽可能的小，这与层级锁的设计思路很相似。

现代操作系统的调度器在做线程调度时主要考虑因素有三点：1）通过线程在核之间的迁移来保证可运行的线程能被调度到可用的核上去；2）保证核之间负载的大致均衡；3）在NUMA架构下使负载均衡算法的代价尽可能大地小。这种调度算法非常适合相互之间没有关系的线程即不通过共享内存通信的线程，它能够避免线程之间对于节点层次的共享资源例如缓存、内存通道等的使用发生冲突（缓存失效等），它的前提假设是线程之间地通信非常少并且远不如节点层次地本地资源使用冲突重要。但是在锁集中并且存在大量线程间共享数据的多线程应用中，显然线程之间的通信是居于主导地位的，保证线程之间通信的高效的重要性要高过线程对资源的独享重要性\cite{dice2015lock}，所以操作系统通用的调度器不能很好的满足这类场景的需求。另外考虑到层级锁中通过挖掘线程之间的亲和性来做锁调度，而通用的调度器将相关的线程分散在所有节点的可用核上使得层级锁可以挖掘的线程间的亲和性非常有限。

综上来看，通用的调度器并不适合NUMA架构下锁集中的多线程应用尤其是使用层级锁多线程的应用，为了使这些应用获得更好的性能，必须有定制化的考虑应用具体特征的线程调度/放置方法，也就是说应该使用定制化的调度器或者让应用程序来做自身的线程调度/放置。


\subsection{NUMA架构下针对锁的线程调度优化}
在NUMA架构下，操作系统的通用调度器很容易造成锁在NUMA节点之间的频繁迁移进而导致其性能下降，所以出现了很多定制化的线程放置/迁移策略，比如最朴素的做法是将请求锁的线程迁移到当前持锁的线程锁在的节点，即将线程迁移到包含锁及其保护的数据的缓存而不是相反，然而这种简单的迁移存在两个问题：1）线程迁移次数不可控；2）容易导致负载不均衡，最终的结果可能是得不偿失。shuffling\cite{pusukuri2014shuffling}是一种能够兼顾负载均衡和迁移代价的适合锁集中的应用的线程放置/迁移策略。shuffling通过将到达时间（lock arrival time）差不多的线程放置在相同的节点上来避免锁在节点间随意迁移，具体做法如图\ref{algo:shuffling}所示，shuffling周期性地收集各个线程的到达时间；然后按照到达时间将所有线程排序分组，到达时间相似的线程被分到相同的组里，每组的大小等于每个节点上的计算核心数；最后通过迁移某些线程将分好的每个线程组映射到NUMA节点上去。相比简单的线程迁移，shuffling保证了负载均衡并且可以很好地控制线程的迁移。

\begin{algorithm}
% \begin{algorithm}[H] % 强制定位
\caption{Shuffling框架}
\label{algo:shuffling}
\begin{algorithmic}[1] %每行显示行号
\Require N:Number of threads, C:Number of Sockets % 输入
\Repeat
\State {\bf i. Monitor Threads} -- sample lock times of N threads
\If{\emph{lock times exceed threshold}}
    \State {\bf ii. Form Thread Groups} -- sort threads according to lock times and divide them into C groups
    \State {\bf iii. Perform Shuffling} -- shuffle threads to establish newly computed thread groups
\EndIf
\Until{application terminates}
\end{algorithmic}
\end{algorithm}

\subsection{现有针对层级锁的线程放置策略}
在层级锁中，上述将请求锁线程迁移到当前持锁线程锁在NUMA节点上显然是没有必要的，因为层级锁并不是按原有锁的传递顺序来传递的。层级锁通过利用线程之间的亲和性来调度锁进而减少锁在节点间的迁移，所以对于层级锁来说最自然最高效的线程放置策略就是在保持负载均衡的前提下将线程尽可能地放得紧凑，所以大多数地层级锁将每个线程绑定到一个专用地核上，只有当前地节点上没有可用的核时才会将新的线程放置在一个新的节点上。此外，也有的层级锁出于测试锁的某些方面的表现等其他目的而将线程平均放置在所有节点上。我们在下一章中会对这两种线程放置策略进行进一步地分析和说明。

\section{锁替换技术}
由于Pthread（POSIX Threads）线程库具有很好的平台可移植性，所以很多历史遗留的多线程应用，比如Memcached，都采用了Pthread线程模型，相应地Pthread mutex也就成了这些应用中最常用的锁。为了在这些应用中测试和使用其他锁并且省去冗长且容易出错的手动替换的麻烦，Hugo Guiroux\cite{guiroux2016multicore}开发了LiTL（Library for Transparent Lock interposition）。

LiTL是一个Linux/x86平台下可以在运行时将基于Pthread mutex的应用中的Pthread mutex替换为其他锁的库。为了实现锁替换，LiTL使用一个可扩展的并发哈希表（CLHT\cite{david2015asynchronized}）维护了一个标准Pthread锁（pthread\_mutex\_t）实例和其他用来替换Pthread mutex的锁实例（比如MCS锁）的映射，也就是说LiTL必须追踪Pthread mutex从pthread\_mutex\_init()到pthread\_mutex\_destroy()的整个生命周期，并且在该生命周期中，每次pthread\_mutex\_lock()都会必须触发一个在上述映射中查找对应其他锁实例的查找操作。此外，某些锁的lock/unlock接口除了锁变量本身以外还需要其他参数，比如在MCS锁中该额外参数对应每个线程的记录（record），对于这些锁，LiTL还在映射中为每个锁实例每个线程维护了一个额外的结构体来表示这些额外参数。

LiTL使用LD\_PRELOAD来拦截基于Pthread mutex的应用中类似pthread\_mutex\_*这样的函数调用，然后查询CLHT，找到对应的映射锁实例，调用对应的函数。LD\_PRELOAD利用了Linux系统中动态链接器（dynamic linker）提供的功能来使用户可以指定动态链接器在加强其他共享库之前绑定某个库的符号（symbol），LiTL提供了与Pthread库相同的外部接口，所以替换Pthread mutex的锁对应的库通过LD\_PRELOAD在应用运行时优先加载后，所有Pthread mutex相关的调用最终都会被拦截到LiTL提供的库中。

\section{本章小结}
本章主要对本文研究所基于的平台和相关技术进行了说明，包括NUMA架构的特性及相关的优化技术，MCS锁、C-MCS锁的实现及特性，Linux系统下的通用调度器的特性及其局限性，还有针对锁集中的应用的一些线程放置策略及其优化，以及锁替换技术等。这些相关的研究和技术一方面是本文研究的基石，另一方面也为本文的研究提供了很多有益的启发和借鉴，比如本文的提出的线程放置框架的混合特性与Carrefour的混合内存放置策略相似，为了保证长期公平性而使用的shift机制与Malthusian锁的shift机制的做法和目的都很相似，而竞争感知的特性则是借鉴了AHMCS锁的竞争感知。